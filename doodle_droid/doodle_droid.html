<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Doodle Droid</title>
  <link rel="stylesheet" href="../styles.css">
  <!-- Load fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Neue+Montreal&family=Rhymes&display=swap" rel="stylesheet">
</head>

<body>
  <!-- Header -->
  <header class="header">
    <div class="container">
      <div class="name">Harrison Bounds</div>
      <nav class="menu">
        <a href="../index.html" class="menu-item">Projects</a>
        <a href="../resume/HarrisonBoundsResume.pdf" class="menu-item">Resume</a>
        <a href="https://github.com/HarrisonBounds" class="menu-item">GitHub</a>
        <a href="https://www.linkedin.com/in/harrison-bounds/" class="menu-item">LinkedIn</a>
        <a href="../about_me.html" class="menu-item">About Me</a>
      </nav>
    </div>
  </header>

  <!-- Project Detail Content -->
  <main class="project-detail">
    <div class="container">
      <!-- Project Title Section -->
      <div class="project-page-header">
        <h1 class="project-page-title">Doodle Droid</h1>
        <p class="project-page-subtitle">ROS2, Python, Manipulation, Motion Planning, Image Processing, MoveIt</p>
        <a href="https://github.com/HarrisonBounds/DoodleDroid" class="github-button">View on GitHub</a>

        <br><br><br>
        <h2>Overview</h2>

        <div class="other-video-container">
          <img src="gif/luffy_doodle_droid_compressed.gif">
          <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Drawing From File</p>
        </div>

        <div>
          <br>
          <div class="project-description">
            <p> Doodle Droid is a 7 DoF arm that detects an image from a camera and processes that image into a drawable
              format. This project leverages ROS 2 to have
              an architecture that can pass messages between nodes with ease. This was a <b>group effort</b>, where the
              work was split up into different nodes. These
              nodes consisted of image processing, calibration, route planning, and motion planning. My main task was
              image processing, but each member in our group helped
              with several aspects of the project. These tasks are explained below.</p>
          </div>

          <br><br>
          <h2>Hardware</h2>

          <div class="image-container" style="text-align: center;">
            <img src="../doodle_droid/franka_datasheet.png" alt="Franka Emika Panda Robot"
              style="width: 400px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Franka Emika Panda Robot</p>
          </div>

          <br>
          <div class="project-description">
            <p>The Franka Emika Panda is a versatile robotic arm known for its precision and user-friendly design.
              With seven joints, it moves like a human arm, making it great for tasks that require dexterity.
              Its built-in torque sensors let it handle delicate jobs safely and accurately.
              Lightweight and compact, the Panda is easy to integrate into projects, whether it’s for research,
              automation, or interactive applications.
              It’s also compatible with ROS 2, making it a go-to choice for modern robotics development.</p>
          </div>

          <br><br>
          <h2>Image Processing</h2>
          <br>

          <div class="image-container" style="text-align: center;">
            <img src="../doodle_droid/yanni_doodle_droid.png" style="width: 300px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Yanni Draws Himself</p>
          </div>

          <br>
          <div class="project-description">
            <p>The usb_cam_node allows the compute webcam publish images over a topic for other nodes to use.
              Starting this node allows for a simple picture taking service, which takes the last image sent on the
              <i>/images</i>
              topic and uses that image. Doodle Droid is also capable of drawing images straight from file, as long as
              its in the correct directory.
              After receiving the image, an external repo is used to process the collected image into a line drawing.
              A link for this GitHub repository can be found <a href="https://github.com/LingDong-/linedraw"
                target="_blank">here</a>.
              In a nutshell, this code converts the pixels into strokes, allowing for a real-time image to become a
              drawing.
            </p>
          </div>


          <br><br>
          <h2>Calibration</h2>
          <br>

          <div class="image-container" style="text-align: center;">
            <img src="../doodle_droid/doodle-droid_april_tag.png" style="width: 200px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">April Tag</p>
          </div>

          <br>
          <div class="project-description">
            <p>The calibration was used to get the <b>z height</b> so the arm knew how much to lower itself. To achieve
              this, our workspace had four of the April tags
              above on the edges of the paper. These position were then averaged to get the center of the paper. With
              this information, we had the option to
              adjust our paper size however we saw fit. Of course drawing smaller images took a shorter time, but larger
              images produced more detail. To detect the april tags,
              we utilized a realsense depth camera by placing the end effector in a position above all of the tags. By
              detecting the tags this way, we could draw our
              image at any orientation, as long as it remained in the franka's workspace</p>
          </div>

          <br><br>
          <h2>Route Planning</h2>
          <br>

          <div class="image-container" style="text-align: center;">
            <img src="../doodle_droid/doodle_droid_image_view.png" style="width: 250px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Line Draw Comparison</p>
          </div>

          <div class="image-container" style="text-align: center;">
            <img src="../doodle_droid/doodle_droid_output.png" style="width: 250px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Route Planning Output</p>
          </div>

          <br>
          <div class="project-description">
            <p>The route planner node receives the "waypoints", produced from the image processor. From here, it
              normalizes these points to the size of our workspace.
              This line data is further converted into 3D waypoints for the robot to follow. This node provides two
              services. The <i>plot</i> service
              visualizes the planned path for the user. The <i>draw</i> service starts the drawing process of the
              processed image. The route planner sends these
              waypoints along to the motion planning node in addition to intermediate commands to pick up and put down
              the drawing utensil.</p>
          </div>

          <br><br>
          <h2>Motion Planning</h2>
          <br>

          <div class="other-video-container">
            <img src="gif/harrison_doodle_droid.gif" style="width: 500px; height: auto;">
            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Drawing From Live Image</p>
          </div>

          <div class="project-description">
            <p>Motion planning is implemented using ROS MoveIt! API, ensuring smooth and precise movements of the
              robotic arm. It can plan several different paths,
              including cartesian, which is the most useful for drawing, since our workspace is constricted to a 2D
              plane (excluding picking up the drawing utensil).
              There is also ample error handling within the motion planning node for debugging purposes. Along with each
              of the joints on the Franka Arm, the motion
              planner also controls the gripper state, which is not required for this particular task.</p>
          </div>


          <!-- Contributors Section -->
          <br><br>

          <div class="contributors">
            <h2>Contributors</h2>
            <ul>
              <a href="https://github.com/CappuccinoPanda" target="_blank">David Matthews</a><br>
              <a href="https://github.com/0nhc" target="_blank">Zhengxiao Han</a><br>
              <a href="https://github.com/Ykechriotis" target="_blank">Yanni Kechriotis</a><br>
              Christian Tongate
            </ul>
          </div>

        </div>
  </main>




  <!-- Scripts -->
  <script src="../filter.js"></script>
</body>

</html>