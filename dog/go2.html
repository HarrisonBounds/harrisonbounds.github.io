<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Go2 RL</title>
    <link rel="stylesheet" href="../styles.css">
    <!-- Load fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Neue+Montreal&family=Rhymes&display=swap" rel="stylesheet">
</head>

<body>
    <!-- Header -->
    <header class="header">
        <div class="container">
            <div class="name">Harrison Bounds</div>
            <nav class="menu">
                <a href="../index.html" class="menu-item">Projects</a>
                <a href="../resume/HarrisonBoundsResume.pdf" class="menu-item">Resume</a>
                <a href="https://github.com/HarrisonBounds" class="menu-item">GitHub</a>
                <a href="https://www.linkedin.com/in/harrison-bounds/" class="menu-item">LinkedIn</a>
                <a href="../about_me.html" class="menu-item">About Me</a>
            </nav>
        </div>
    </header>

    <!-- Project Detail Content -->
    <main class="project-detail">
        <div class="container">
            <!-- Project Title Section -->
            <div class="project-page-header">
                <h1 class="project-page-title">Teaching an Old Dog New Tricks</h1>
                <p class="project-page-subtitle">Reinforcement Learning, Proximal Policy Optimzation, Legged Locomotion,
                    Simulation</p>
                <a href="https://github.com/HarrisonBounds/go2RL" class="github-button">View on GitHub</a>

                <br><br><br>
                <h2>Overview</h2>

                <div class="other-video-container">
                    <video autoplay muted loop style="width: 600px !important; height: 400px !important;">
                        <source src="../dog/go2_running.webm" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Faster!</p>
                </div>


                <div>
                    <br>
                    <div class="project-description">
                        <p> This project aims to train the Unitree Go2 Robot dog to perform a variety of tasks including
                            walking, running, jumping, strafing, and crawling on rough terrain. To train the policy, we
                            use an Actor-Critic network structure, integrated into a Proximal Polixy Optimzation (PPO)
                            algorithm. To simulate this policy, we used a lightweight, new physics simulator called
                            Genesis, which is greatfor training locomotion policies on legged robots. Sim-to-real is
                            still in the works for this project, but we are hoping to have it working soon!
                        </p>
                    </div>

                    <br><br>
                    <h2>Hardware</h2>

                    <div class="image-container" style="text-align: center;">
                        <img src="../dog/go2_dog.jpeg" style="width: 400px; height: auto;">
                        <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Unitree Go2 Dog
                        </p>
                    </div>


                    <br>
                    <div class="project-description">
                        <p>The Unitree Go2 Robot Dog is a quadpred (of course)
                            that has three joints per leg. This creates an action space of 16 possible actions which is
                            the output of the actor network. The go2's urdf is open-source, and is what we use to
                            visualize it in the environment. For a baseline, we start with the Genesis example with the
                            go2 walking on a flat plane.
                        </p>
                    </div>

                    <br><br>
                    <h2>Proximal Policy Optimization</h2>
                    <br>


                    <br><br>
                    <div class="project-description">
                        <p>PPO starts by intializing the actor and critic networks. Each of the inputs are the
                            observation space while the output is the number of actions and the expected reward function
                            respectively. After sampling an action from the outputted distribution, the critic network
                            predicts the expected reward and the experiences are stored. These steps are repeated as a
                            type of "data collection" phase.

                            When the model has enough data, we can begin to learn a policy. We calculate new actions and
                            evaluate the state space. A policy ratio is then computed to give the probability of the
                            action being chosen for the current policy. This ratio is used in the surrogate loss
                            function, which also includes an advantages term to see how much better the current policy
                            is from the last. Finally, we intergrate the "proximal" portion of the algorithm by clipping
                            the value between 1-ε and 1+ε so the policy does not stray too far. This concludes the PPO
                            algorithm, you can find the equations below.
                        </p>
                    </div>

                    <div class="project-description">

                        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                        <script id="MathJax-script" async
                            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

                        <center>
                            <h4>Objective Function</h4>
                        </center>
                        <p>
                            \[
                            L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t,
                            \text{clip}(r_t(\theta), 1 - \epsilon, 1 +
                            \epsilon) \hat{A}_t \right) \right]
                            \]
                            where:
                        <ul>
                            <li>\( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \) is
                                the probability ratio,
                            </li>
                            <li>\( \hat{A}_t \) is the advantage estimate at time \( t \),</li>
                            <li>\( \epsilon \) is a hyperparameter (e.g., 0.2) that controls the clipping range.</li>
                        </ul>
                        </p>
                    </div>

                    <div class="project-description">
                        <center>
                            <h4>Advantage Estimation</h4>
                        </center>
                        <p>
                            \[
                            \hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
                            \]
                            where:
                        <ul>
                            <li>\( \gamma \) is the discount factor,</li>
                            <li>\( \lambda \) is the GAE parameter,</li>
                            <li>\( \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \) is the TD residual.</li>
                        </ul>
                        </p>
                    </div>

                    <div class="project-description">
                        <center>
                            <h4>Value Function Loss (MSE)</h4>
                        </center>
                        <p>
                            \[
                            L^{VF}(\theta) = \mathbb{E}_t \left[ \left( V_\theta(s_t) - V_t^{\text{target}} \right)^2
                            \right]
                            \]
                            where \( V_t^{\text{target}} \) is the target value (e.g., discounted return).
                        </p>
                    </div>

                    <div class="project-description">
                        <center>
                            <h4>Entropy Bonus</h4>
                        </center>
                        <p>
                            \[
                            L^{ENT}(\theta) = \mathbb{E}_t \left[ -\pi_\theta(a_t|s_t) \log \pi_\theta(a_t|s_t) \right]
                            \]
                        </p>
                    </div>

                    <div class="project-description">
                        <center>
                            <h4>Total Loss</h4>
                        </center>
                        <p>
                            \[
                            L^{TOTAL}(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 L^{ENT}(\theta)
                            \]
                        </p>
                    </div>




                    <br><br>
                    <h2>Reward Function Engineering</h2>
                    <br>

                    <br>
                    <div class="project-description">
                        <p>To make create an optimal policy for each task, we need to create custom reward functions
                            that
                            encourage the correct behavior. For example, a common running gait for a robot quadpred is
                            the
                            diagonal gait, where the front right leg and the back rear leg move together, and vice
                            versa. To
                            achieve this, a custom reward function was needed to penalize FL and FR correlation. Below
                            is a
                            list that includes, but isnt limited to, custom reward functions that were added for
                            different
                            tasks.

                            <center>
                                Diagonal Gait <br>
                                Forward Velocity <br>
                                Aligned Hips <br>
                                Base height <br>
                                Action Rate <br>
                            </center>
                        </p>
                    </div>

                    <br><br>
                    <h2>Simulation</h2>
                    <br>

                    <div class="image-container" style="text-align: center;">
                        <img src="../dog/genesis.png" style="width: 300px; height: auto;">
                        <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Genesis Simulator
                        </p>
                    </div>

                    <br>
                    <div class="project-description">
                        <p>To simulate the go2 interacting with the environment, we used a platform called Genesis,
                            which is
                            a lightweight physics simulator perfect for training locomotion policies for legged robots.
                            As
                            said earlier, we use the open-source unitree go2 urdf to model the robot, and a flat plane
                            to
                            perform various tasks on (although some tasks have different terrain). By interacting with
                            this
                            environment, our robot gave record state spaces, rewards, and other experiences.

                            <br><br>

                            The algorithm used is the PPO (Proximal Policy Optimization) algorithm. In a nutshell,
                            This algorithm takes in the observation space as an input to a neural network, and outputs
                            a probability of likely actions to maxmize the reward. There is also another network that
                            has the
                            same input, and output the likely reward to be receieved. This Actor-Critic relationship
                            trains the hexapod
                            to walk!
                        </p>

                    </div>

                    <br><br>
                    <h2>Results</h2>
                    <br>



                    <div class="project-description">
                        <p>Our results focus on mean rewards per episode, as we found that this metric most accurately
                            encapsulates the success of the policy.</p>

                    </div>

                    <div class="project-description">
                        <h4>Baseline Results</h4>

                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/go2_baseline.webm" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Walking
                                Simulation</p>
                        </div>

                        <br>
                        <div class="image-container" style="text-align: center;">
                            <img src="../dog/baseline_results.png" style="width: 700px; height: auto;">
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Walking: ~16
                            </p>
                        </div>
                    </div>

                    <div class="project-description">
                        <h4>Our Results</h4>

                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/climbStairs.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Climbing
                                Simulation</p>
                        </div>

                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/jumpInPlace.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Jumping
                                Simulation</p>
                        </div>

                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/jumpForward.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Jumping
                                Forward
                                Simulation</p>
                        </div>


                        <br>
                        <div class="image-container" style="text-align: center;">
                            <img src="../dog/climb_jump_results.png" style="width: 800px; height: auto;">
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Climbing: ~53,
                                Jumping: ~20, Jumping Forward: ~22,
                            </p>
                        </div>
                    </div>



                    <div class="project-description">

                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/strafing_video.webm" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Strafing
                                Simulation</p>
                        </div>

                        <div class="image-container" style="text-align: center;">
                            <img src="../dog/strafing.png" style="width: 900px; height: auto;">
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Strafing:
                                ~16
                            </p>
                        </div>
                    </div>

                    <div class="project-description">
                        <div class="other-video-container">
                            <video autoplay muted loop style="width: 600px !important; height: 600px !important;">
                                <source src="../dog/go2_running.webm" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Running
                                Simulation</p>
                        </div>
                        <div class="image-container" style="text-align: center;">
                            <img src="../dog/Running.png" style="width: 800px; height: auto;">
                            <p class="caption" style="margin-top: 10px; font-style: italic; color: gray;">Running:
                                ~146
                            </p>
                        </div>
                    </div>


                    <div class="contributors">
                        <div class="project-description">
                            <h2>Contributors</h2>
                            <ul>
                                <a href="https://github.com/rdlynx19" target="_blank">Pushkar Dave</a><br>
                                <a href="https://github.com/Sharwin24" target="_blank">Sharwin Patil</a><br>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>



        </div>
    </main>




    <!-- Scripts -->
    <script src="../filter.js"></script>
</body>

</html>